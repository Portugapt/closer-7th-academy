{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"7th Academy Pratical Execises","text":"<ul> <li>7th Academy Pratical Execises</li> <li>Architecture<ul> <li>Ingestion and training</li> <li>Make the model available</li> </ul> </li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>We are going to build a simple MLOps architecture in Google Cloud Platform using <code>Cloud Storage</code>, <code>Cloud Functions</code>, <code>Bigquery</code> and <code>Pubsub</code>.</p> <p>Our minimal MLOps system should look like this in the end:</p> <p></p>"},{"location":"#ingestion-and-training","title":"Ingestion and training","text":"<ol> <li>Cloud Function <code>ingest_data</code> monitors the <code>my-data-landing-zone</code> for new files.</li> <li>Upon detecting a new file, <code>ingest_data</code> writes its contents to the BigQuery table <code>training_data</code>.</li> <li>A message is sent to the <code>ingestion_complete</code> topic, notifying subscribers about the new data in BigQuery.</li> <li>The <code>train_model</code> Cloud Function, subscribed to <code>ingestion_complete</code>, is triggered and begins training.</li> <li>It retrieves data from the <code>training_data</code> BigQuery table.</li> <li>The trained model is saved in the <code>my-model-storage</code> bucket.</li> </ol>"},{"location":"#make-the-model-available","title":"Make the model available","text":"<ol> <li>The <code>predictions_endpoint</code> Cloud Function receives a request containing new data from a client.</li> <li>The Function loads the previously stored model into memory.</li> <li>It makes a prediction and stores the prediction and new data in the <code>predictions_data</code> BigQuery table.</li> <li>The prediction result is returned to the client.</li> </ol>"},{"location":"client_libs_py/","title":"Client Libraries","text":"<p>A client implementation, such as a Python Client or Javascript Client, is a software library designed to facilitate communication and interaction between an application and a specific service, like an API. It allows developers to easily access and utilize the service's functionalities, by abstracting low-level details and providing a more user-friendly interface in the language of choice.</p> <p>The client implementation acts as an API layer between the application and the server, enabling seamless data exchange and requests management. This layer simplifies the process of making API calls, handling authentication, managing connection details, and processing responses from the server.</p> <p>For example, the Google Cloud Platform (GCP) offers BigQuery Python Client and Python Cloud Storage as part of their Cloud Client Libraries. These libraries provide high-level API abstractions that significantly reduce the amount of boilerplate code developers need to write when interacting with BigQuery and Cloud Storage services.</p> <p>Using the GCP BigQuery Python Client, developers can easily query, manage, and load data into BigQuery tables, while the Python Cloud Storage library simplifies file management, uploads, and downloads in Google Cloud Storage. Both libraries embrace the idiomatic style of the Python language, ensuring better integration with the standard library and developers' existing codebases.</p> <p>You can check all the available Client Libraries for python here.</p>"},{"location":"client_libs_py/#bigquery-client-python","title":"Bigquery Client (Python)","text":"<p>You can use the BigQuery Python Client to execute a query and fetch the results:</p> <pre><code># NOTE: pip install google-cloud-bigquery\n\nfrom google.cloud import bigquery\n\n# Initialize the BigQuery client\nclient = bigquery.Client()\n\n# Define your query\nquery = \"\"\"\n    SELECT name, SUM(number) as total\n    FROM `bigquery-public-data.usa_names.usa_1910_current`\n    WHERE year &gt;= 2000\n    GROUP BY name\n    ORDER BY total DESC\n    LIMIT 10\n\"\"\"\n\n# Execute the query\nquery_job = client.query(query)\n\n# Fetch and print the results\nfor row in query_job.result():\n    print(f\"{row.name}: {row.total}\")\n</code></pre>"},{"location":"client_libs_py/#cloud-storage-client-python","title":"Cloud Storage Client (Python)","text":"<p>You can use the Python Cloud Storage Client to upload a file to a GCS bucket and download it back:</p> <pre><code># NOTE: pip install google-cloud-storage\n\nfrom google.cloud import storage\n\n# Initialize the GCS client\nclient = storage.Client()\n\n# Specify your bucket name\nbucket_name = \"your-bucket-name\"\n\n# Get a reference to the bucket\nbucket = client.get_bucket(bucket_name)\n\n# Upload a file\nsource_file_name = \"path/to/your/local/file.txt\"\ndestination_blob_name = \"uploaded_file.txt\"\nblob = bucket.blob(destination_blob_name)\nblob.upload_from_filename(source_file_name)\nprint(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n\n# Download the file\ndownloaded_file_name = \"path/to/your/local/downloaded_file.txt\"\nblob = bucket.blob(destination_blob_name)\nblob.download_to_filename(downloaded_file_name)\nprint(f\"File {destination_blob_name} downloaded to {downloaded_file_name}.\")\n</code></pre>"},{"location":"client_libs_py/#pubsub-client-python","title":"Pub/Sub Client (Python)","text":"<p>You can use the Pub/Sub Python Client to publish a message to the existing topic:</p> <pre><code># NOTE: pip install google-cloud-pubsub\n\nfrom google.cloud import pubsub_v1\n\n# Initialize the Pub/Sub client\npublisher = pubsub_v1.PublisherClient()\n\n# Set your project_id and topic_name\nproject_id = \"your-project-id\"\ntopic_name = \"your-existing-topic-name\"\n\n# Get the topic path\ntopic_path = publisher.topic_path(project_id, topic_name)\n\n# Publish a message\nmessage = \"Hello, World!\"\nmessage_data = message.encode(\"utf-8\")\nfuture = publisher.publish(topic_path, message_data)\nmessage_id = future.result()\nprint(f\"Message published with ID: {message_id}\")\n</code></pre> <p>As Pub/Sub promotes decoupled and flexible architectures, message_data is transformed into a base64-encoded string to ensure language-agnostic compatibility. Therefore, subscribers must decode the base64 message. In Python, this can be done as follows:</p> <pre><code>import base64\n\ndef hello_pubsub(data, context):\n\"\"\"Triggered from a message on a Cloud Pub/Sub topic.\n    Args:\n         event (dict): Event payload.\n         context (google.cloud.functions.Context): Metadata for the event.\n    \"\"\"\n    print(\"This Function was triggered by messageId {} published at {}\".format(context.event_id, context.timestamp))\n\n    if 'data' in event:\n        decoded_msg = base64.b64decode(data['data']).decode('utf-8')\n        # Message is now decoded\n    ## Your Cloud Function Implementation\n</code></pre>"},{"location":"client_libs_py/#setup-python-venv","title":"Setup python venv","text":"<pre><code>python -m venv venv\n</code></pre> <p>Install the Python Extension:</p> <p></p> <p>With VSCode, do <code>CTRL+SHIFT+P</code> and write <code>Select Interpreter</code></p> <p></p> <p>And find the <code>venv</code> python executable.</p> <p></p>"},{"location":"create_resources/","title":"Create Resources (Tables/Buckets/Topics) within GCP","text":""},{"location":"create_resources/#buckets-ui","title":"Buckets (UI)","text":"<ol> <li> <p>Search for the Cloud Storage in the Search bar.</p> <p></p> </li> <li> <p>In the Cloud Storage UI, you'll notice there are no buckets created yet. To create one, click the <code>CREATE</code> button.</p> <p></p> </li> <li> <p>Configurate your bucket</p> <p></p> <ol> <li>Name your bucket and click Continue.</li> <li>Change the storage class from Multi-region to Region. Set the location to europe-west3, as shown in the image, and click Continue.</li> <li>Keep the remaining settings as they are.</li> <li>Click create.</li> </ol> <p>Your configuration should look like this:</p> <p></p> <p>If this popup appears, leave the settings as they are.</p> <p></p> </li> </ol> <p>And now you have your bucket!</p> <p></p> <p>Alternatively, you can create a bucket using Python, other Client Libraries, or even advanced Infrastructure-as-Code tools like Terraform or Pulumi.</p>"},{"location":"create_resources/#what-is-google-cloud-storage-gcs","title":"What is Google Cloud Storage (GCS)?","text":"<p>Google Cloud Storage (GCS) is a scalable, fully-managed, and highly available object storage service provided by Google Cloud Platform. It allows users to store, access, and manage data across multiple storage classes, catering to various use cases like backup, archival, and content delivery. GCS ensures data durability and offers seamless integration with other Google Cloud services.</p>"},{"location":"create_resources/#bigquery-data-sets-and-tables-ui","title":"Bigquery Data Sets and Tables (UI)","text":"<p>Tables are always associated with a <code>data set</code>. First, let's create a <code>data set</code>.</p> <ol> <li>Go to BigQuery:</li> </ol> <p></p> <ol> <li>Click the bullet points icon next to the project name:</li> </ol> <p></p> <ol> <li>Name your data set, change the region, and click <code>CREATE DATA SET</code>:</li> </ol> <p></p> <pre><code>Congratulations! You have a `data set`!\n\nNow, let's create a table:\n</code></pre> <ol> <li>Click the bullets icon next to your data set, and click Create Table:</li> </ol> <p></p> <ol> <li>Configure your table settings:</li> </ol> <p></p> <pre><code>Alternatively, you can define the schema using `JSON`:\n\n![bq-6](https://i.imgur.com/UcDK3uC.png)\n</code></pre> <p>And now you have a table too!</p> <p>Remember the location of the Table ID; you might need it later:</p> <p></p> <p>Learn more about tables in the documentation.</p> <p>You can also create Tables with Infrastructure-As-Code tools. Here are the examples for Terraform and for the several Clients for Pulumi</p>"},{"location":"create_resources/#what-is-bigquery","title":"What is Bigquery?","text":"<p>BigQuery is a fully-managed, serverless, petabyte-scale data warehouse by Google Cloud Platform. It enables super-fast SQL queries using the processing power of Google's infrastructure, allowing users to analyze large datasets in real-time. BigQuery is designed for scalability, ease of use, and integration with other Google Cloud services.</p>"},{"location":"create_resources/#json-schema","title":"JSON Schema","text":"<p>Why should you use JSON schemas when possible?</p> <ol> <li> <p>Structure and consistency: JSON schemas define the structure of your data, ensuring consistency across all records in the table. This helps maintain data integrity and makes it easier to query and analyze the data.</p> </li> <li> <p>Validation: By specifying a schema, you can enforce data validation rules, such as data types and required fields, ensuring that only valid data is inserted into the table. This can prevent issues caused by incorrect or incomplete data.</p> </li> <li> <p>Readability: JSON schemas provide a clear and human-readable description of the table's structure, making it easier for team members to understand the data and its organization</p> </li> <li> <p>Interoperability: JSON schemas are a standardized format, which makes it simpler to share and exchange table structures across teams and different systems. This is particularly useful when integrating with other tools or platforms that support JSON schema.</p> </li> <li> <p>Easier data import: When importing data from files (e.g., CSV or JSON) into BigQuery, providing a JSON schema allows BigQuery to map the file's data correctly to the table's columns, preventing import errors and ensuring data consistency.</p> </li> </ol> <p>Here's an example of a JSON schema:</p> <pre><code>[\n{\n\"name\": \"my_required_text_field\",\n\"type\": \"STRING\",\n\"mode\": \"REQUIRED\",\n\"description\": \"A text field\"\n},\n{\n\"name\": \"my_required_integer_field\",\n\"type\": \"INTEGER\",\n\"mode\": \"NULLABLE\",\n\"description\": \"An integer field\"\n},\n{\n\"name\": \"my_nullable_boolean_field\",\n\"type\": \"BOOLEAN\",\n\"mode\": \"NULLABLE\",\n\"description\": \"A boolean field\"\n}\n]\n</code></pre> <p>You can also find it in the folder <code>./infrastructure/bigquery/example_schema.json</code>.</p> <p>You can fiend more about how to define JSON schemas in the Google Documentation.</p>"},{"location":"create_resources/#pubsub-topics-ui","title":"Pub/Sub Topics (UI)","text":"<ol> <li>Search for Topics in the search bar.</li> <li> <p>Click in CREATE TOPIC.</p> <p></p> </li> <li> <p>Define your Topic ID and click CREATE</p> <p></p> <p>In this case, our Topic ID is <code>ingestion_complete</code>.</p> <p>Remember where to find your Topic IDs, it will be useful when instrumenting the python scripts.</p> </li> <li> <p>We have a new topic!</p> </li> </ol> <p></p> <p>It automatically creates a subscription, but lets ignore that for now.</p> <ol> <li> <p>If you go back to the Topics page, it should look like this</p> <p></p> </li> </ol> <p>Like any other resource, we can also create Topics and Subscriptions with IaC.</p> <p>Follow these links for examples for Terraform and Pulumi.</p>"},{"location":"create_resources/#what-is-the-publisher-subscriber-pattern","title":"What is the Publisher-Subscriber pattern?","text":"<pre><code>  The publisher-subscriber (pub-sub) messaging pattern is a communication paradigm where messages are sent by publishers to multiple subscribers, without requiring direct connections between them. Publishers broadcast messages to topics, and subscribers listen to topics they are interested in. This pattern provides a decoupled architecture, allowing for scalability, flexibility, and fault tolerance. Subscribers receive messages asynchronously, enabling them to process events independently, without blocking or waiting for other subscribers. The pub-sub pattern is widely used in distributed systems, event-driven architectures, and messaging applications.\n</code></pre>"},{"location":"create_resources/#what-is-google-pubsub","title":"What is Google Pub/Sub?","text":"<pre><code>  Google Pub/Sub is a real-time messaging service based on the publisher-subscriber pattern, designed for Google Cloud Platform. It enables reliable, scalable, and asynchronous event-driven communication between microservices, applications, and data streams, promoting decoupled and flexible architectures.\n</code></pre>"},{"location":"create_resources/#deploy-cloud-functions-gcloud","title":"Deploy Cloud Functions (gcloud)","text":"<p>In the exercises we will deploy Cloud Functions from a Zip file. For this, we need a bucket specifically for storing the zipped code of the functions. Find more information on GCP documentation, and follow the example in how to create a bucket to create a bucket for the zipped files.</p> <ol> <li> <p>Activate the Cloud Shell</p> <p></p> <p>After a while, you should have a command line in the bottom of your browser. Confirm that you have an active project (green rectangle). If not, contact us.</p> <p></p> </li> <li> <p>Execute the following command:</p> <pre><code>gcloud functions deploy [YOUR_FUNCTION_NAME] \\\n--region=europe-west3 \\\n--runtime=python39 \\\n--source=gs://[ZIPPED_FUNCTIONS_BUCKET]/[ZIP_NAME] \\\n--entry-point=main \\\nTRIGGER_FLAGS\n</code></pre> <p><code>--region</code>: Deployment Region. - List of Regions</p> <p><code>--runtime</code>: The execution environment. - Available environments (We will use python39)</p> <p><code>--source</code>: Source code of the function. - There are several ways to access the source code. We will use Deploy from Cloud Storage. But you can deploy from a source repository or directly from a local machine (your PC).</p> <p><code>--entry-point</code>: The function/code executed when the Cloud Function runs. - Learn more here for event-driven functions and for http functions.</p> <p><code>TRIGGER_FLAGS</code>: The trigger type of the Cloud Function. - See more in the table here. - In our case, we will use three trigger types for the three cloud functions. <code>--trigger-bucket</code>, <code>--trigger-topic</code> and <code>--trigger-http</code>.</p> <p>If this shows up:</p> <p></p> <p>Type <code>y</code></p> </li> <li> <p>Check the status of your deployment.</p> <p>Search for Cloud Build</p> <p></p> <p>And check if you are in Region europe-west3. Something similar to this should show up:</p> <p></p> <p>If the cloud function was deployed with success, you'll get a green check</p> <p></p> <p>Go to the Cloud Functions UI, and check that your function was deployed with success.</p> <p></p> <p>You can click the function name and check it's properties, configurations and even source code deployed.</p> <p>For example, we can check if the trigger of a cloud function in the <code>Trigger</code> page.</p> <p></p> <p>If your deployment didn't work, let us know and we'll help you.</p> </li> </ol>"},{"location":"manual_exercises/ex1/","title":"Load a file from Cloud Storage to a Bigquery Table using a Cloud Function.","text":"<p>In this exercise, we will create the <code>ingest_data</code> Cloud Function, that will perform the following tasks:</p> <ol> <li> <p>The <code>ingest_data</code> function will actively monitor the <code>my-data-landing-zone</code> Google Cloud Storage bucket for new files. This is achieved by configuring a trigger in the Cloud Function to listen for object creation events in the specified bucket.</p> </li> <li> <p>When a new file is detected, the <code>ingest_data</code> function will read the contents of the file and write the data into a BigQuery table named <code>training_data</code>. The function will leverage the BigQuery Python client library to facilitate this process, efficiently importing the data from the file into the specified table.</p> </li> <li> <p>After successfully importing the data into BigQuery, the <code>ingest_data</code> function will send a message to the <code>ingestion_complete</code> topic in Google Cloud Pub/Sub. This message will notify all subscribers that new data has been loaded into BigQuery, allowing them to react accordingly, such as by initiating further data processing tasks.</p> </li> </ol> <p>The Cloud Function <code>ingest_data</code> will utilize the Google Cloud Storage, BigQuery, and Pub/Sub client libraries for these tasks. Our goal in this exercise is to develop the code for this function and deploy it to Google Cloud Platform.</p> <ul> <li>You can adapt the function to create flags/categories for TRAIN/TEST/VALIDATION at runtime, assuming your table was created with that field.</li> </ul> <p>For this you will need these resources:</p> <ul> <li>One Bigquery <code>data set</code> and one bigquery <code>table</code> (The initial schema is available at <code>./infrastructure/bigquery/titanic_schema.json</code>)</li> <li>One GCS Bucket named <code>[prefix]-landing-zone-bucket</code> where you will drop the files once the function is ready</li> <li>One GCS Bucket named <code>[prefix]-functions-bucket</code> where you will deploy the function source code from.</li> <li>One Topic named <code>[prefix]-ingestion-complete</code>, to where the function will send a message once complete.</li> </ul> <p>The outline of the Cloud Function code is available at <code>./functions/manual_exercises/ingest_data/</code>.</p> <p>Here are the steps you should follow:</p> <ol> <li>Create Clients: Use the Google Cloud Storage API, BigQuery API, and PubSub API to create respective client objects.</li> </ol> <pre><code># INSTRUMENTATION [1]: Use the storage API to make a Client Object\n# INSTRUMENTATION [2]: Use the bigquery API to make a Client Object\n# INSTRUMENTATION [3]: Use the pubsub_v1 API to make a PublisherClient Object\n</code></pre> <ol> <li>Set Environment Variables: Set your project configurations like project ID, dataset ID, table name, and topic ID.</li> </ol> <pre><code># IMPLEMENTATION [4]: Set your configurations here\n</code></pre> <ol> <li>Insert Rows into BigQuery: Find the correct method to insert rows as JSON into the BigQuery table.</li> <li>Hint: Find all the bigquery <code>Client()</code> methods here</li> </ol> <p><pre><code># IMPLEMENTATION [5]: Find the correct method to use here\n</code></pre> 4 . Publish Message: Find the correct method with the PublisherClient to publish a message.     - Hint: PublisherClient</p> <pre><code># IMPLEMENTATION [6]: Find the correct method with the PublisherClient to publish a message\n</code></pre> <ol> <li>(Optional) Assign Set Types: You can define a train/test/validation column here. Define that column in your BigQuery table too.</li> </ol> <pre><code># OPTIONAL [1]: You can define a train / test / validation column here. Define that column in your BigQuery table too.\n</code></pre> <p>Deployment:</p> <pre><code>gcloud functions deploy prefix_ingest_data \\\n--region=europe-west3 \\\n--runtime=python39 \\\n--source=gs://prefix-functions-bucket/ingest_data.zip \\\n--entry-point=main \\\n--trigger-bucket=prefix-landing-bucket\n</code></pre>"},{"location":"manual_exercises/ex1/#code","title":"Code:","text":"<p>Remember, you can still find it in the correct folder.</p>"},{"location":"manual_exercises/ex1/#manual_exercises.ingest_data.main","title":"<code>main</code>","text":""},{"location":"manual_exercises/ex1/#manual_exercises.ingest_data.main.main","title":"<code>main(event_data, context)</code>","text":"<p>Entrypoint of the cloud function</p> <p>Parameters:</p> Name Type Description Default <code>event_data</code> <code>dict</code> <p>Event payload</p> required <code>context</code> <code>dict</code> <p>Event context.</p> required <p>The --trigger-bucket event_data is the following: https://github.com/googleapis/google-cloudevents/blob/main/proto/google/events/cloud/storage/v1/data.proto</p> <pre><code>event_data: {\nstorageClass: string\nsize: string\nid: string\nselfLink: string\ntimeStorageClassUpdated: Timestamp\nupdated: Timestamp\ncrc32c: string\ngeneration: string\ntimeCreated: Timestamp\nmediaLink: string\netag: string\nname: string\nbucket: string\nmd5Hash: string\nmetageneration: string\ncontentType: string\nkind: string\n}\n</code></pre> <p>The important keys to us now are <code>name</code>, which is the name of the file that triggered the event, and <code>bucket</code>, which is the bucket this cloud function is listening.</p> Source code in <code>functions/manual_exercises/ingest_data/main.py</code> <pre><code>def main(event_data, context):\n\"\"\"Entrypoint of the cloud function\n\n    Args:\n        event_data (dict): Event payload\n        context (dict): Event context.\n\n    The --trigger-bucket event_data is the following:\n    https://github.com/googleapis/google-cloudevents/blob/main/proto/google/events/cloud/storage/v1/data.proto\n\n    ```json\n    event_data: {\n        storageClass: string\n        size: string\n        id: string\n        selfLink: string\n        timeStorageClassUpdated: Timestamp\n        updated: Timestamp\n        crc32c: string\n        generation: string\n        timeCreated: Timestamp\n        mediaLink: string\n        etag: string\n        name: string\n        bucket: string\n        md5Hash: string\n        metageneration: string\n        contentType: string\n        kind: string\n    }\n    ```\n\n    The important keys to us now are `name`, which is the name of the file\n    that triggered the event, and `bucket`, which is the bucket this\n    cloud function is listening.\n    \"\"\"\n    # Clients\n    # storage_client = # IMPLEMENTATION [1]: Use the storage API to make a Client Object\n    # bigquery_client = # IMPLEMENTATION [2]: Use the bigquery API to make a Client Object\n    # publisher = # IMPLEMENTATION [3]: Use the pubsub_b1 API to make a PubliserClient Object\n\n    # Environment variables\n    # Note: In a real environment these variables would be passed by environment variables.\n    # See: https://cloud.google.com/sdk/gcloud/reference/functions/deploy#--env-vars-file\n    project_id: str = \"Your project ID\" # IMPLEMENTATION [4]: Set your configurations here\n    dataset_id: str = \"Your Data set ID\" # IMPLEMENTATION [4]: Set your configurations here\n    table_name: str = \"Your Table ID\" # IMPLEMENTATION [4]: Set your configurations here\n    topic_ingestion_complete = \"Your Topic ID\" # IMPLEMENTATION [4]: Set your configurations here\n\n    # Get a reference to the bucket\n    bucket: storage.Bucket = storage_client.get_bucket(event_data['bucket'])\n    # The key 'bucket' exists in the event_data\n\n    # The ID of your new GCS object\n    blob: storage.Blob = bucket.blob(event_data['name'])\n\n    # Iterate over the file\n    with blob.open(\"r\") as f:  # Link [1]\n        # Read the entire content of the file\n        content: str = f.read()\n\n    # Split the content by lines\n    lines: List[str] = content.strip().split('\\n')\n\n    # Get the header (column names) from the first line\n    headers: List[str] = lines[0].split(',')\n\n    # Iterate through the rest of the lines (the data points)\n    for datapoint in lines[1:]:\n        errors = bigquery_client._( # IMPLEMENTATION [5]: Find the correct method to use here\n            table=f\"{dataset_id}.\",\n            json_rows=[_transform_datapoint_into_dictionary(\n                headers=headers,\n                datapoint=datapoint)],\n        )\n        if errors:\n            print(json.dumps({\n                \"message\": \"Encountered errors while inserting row\",\n                \"errors\": errors,\n                'data': _transform_datapoint_into_dictionary(\n                    headers=headers,\n                    datapoint=datapoint),\n                \"severity\": \"ERROR\",\n            }))\n\n    # Publish the message\n    # Define the topic path, it's a string \"projects/[PROJECT_ID]/topics/[TOPIC_ID]\"\n    # but the `topic_path` method helps us.\n    topic_path: str = publisher.topic_path(\n        project_id, topic_ingestion_complete)\n    data = f\"I finished ingesting the file {event_data['name']}!!\"\n</code></pre>"},{"location":"manual_exercises/ex2/","title":"Deploy a Cloud function that trains a model and saves it in GCS.","text":"<p>In this exercise, we will create a Cloud Function called <code>train_model</code>, which will be responsible for training a machine learning model using the data ingested in the previous steps. The function will be triggered by the <code>ingestion_complete</code> Pub/Sub topic, ensuring it starts training once new data is available in the BigQuery table. The steps involved in this process are as follows:</p> <ol> <li> <p>The <code>train_model</code> Cloud Function is subscribed to the <code>ingestion_complete</code> topic, and it will be triggered automatically when a new message is published, indicating that new data has been loaded into the BigQuery table.</p> </li> <li> <p>Upon being triggered, the <code>train_model</code> function retrieves the data from the <code>training_data</code> BigQuery table using the appropriate query. This data will be used to train a machine learning model, such as a Scikit-learn Random Forest or Logistic Regression model.</p> </li> <li> <p>After the model is trained using the fetched data, the <code>train_model</code> function saves the trained model to the <code>my-model-storage</code> Google Cloud Storage bucket. The user implementing this function can choose the preferred naming convention for the saved model.</p> </li> </ol> <p>This exercise will guide you through the process of developing the <code>train_model</code> Cloud Function, which leverages the power of BigQuery, Scikit-learn, and Google Cloud Storage to create, train, and store a machine learning model.</p> <p>For this you will need these resources:</p> <ul> <li>One Bigquery <code>data set</code> and one bigquery <code>table</code> (The initial schema is available at <code>./infrastructure/bigquery/titanic_schema.json</code>)</li> <li>One GCS Bucket named <code>[prefix]-models-bucket</code> where you will save the model</li> <li>One GCS Bucket named <code>[prefix]-functions-bucket</code> where you will deploy the function source code from.</li> <li>One Topic named <code>[prefix]-ingestion-complete</code>, to which the function will be subscribed to.</li> </ul> <p>The outline of the Cloud Function code is available at <code>./functions/manual_exercises/train_model/</code></p> <ol> <li>Decode Base64 Message: Add code to decode the base64 message.</li> </ol> <pre><code># IMPLEMENTATION [1]: Add code to decode the base64 message.\n</code></pre> <ol> <li>Create Clients: Use the Google Cloud Storage API and BigQuery API to create respective client objects.</li> </ol> <pre><code># IMPLEMENTATION [1]: Use the storage API to make a Client Object\n# IMPLEMENTATION [2]: Use the bigquery API to make a Client Object\n</code></pre> <ol> <li>Create SQL Query: Create an SQL query to retrieve data from the BigQuery table with Titanic data.</li> </ol> <pre><code># IMPLEMENTATION [3]: Create an SQL query to retrieve data from the bigquery table with Titanic data.\n</code></pre> <ol> <li>Set Bucket Name: Add your GCS bucket name to store the trained model.</li> </ol> <pre><code># IMPLEMENTATION [4]: Add your prefix-bucket-models here.\n</code></pre> <ol> <li>Set Model Name: Give a name to your trained model.</li> </ol> <pre><code># IMPLEMENTATION [5]: Give a name to your model.\n</code></pre> <ol> <li>Connect to Bucket: Connect to the GCS bucket using the correct method for the Storage Client.</li> </ol> <pre><code># IMPLEMENTATION [6]: Connect to the bucket in [4] using the correct method\n</code></pre> <ol> <li>Connect to Blob: Connect to the blob (file object) inside the bucket, using the bucket object.</li> </ol> <pre><code># IMPLEMENTATION [7]: Connect to the blob(file object) inside the bucket, using the `bucket` object.\n</code></pre> <ol> <li>(Optional) Remove Columns: Remove any additional columns that shouldn't be passed to the model.</li> </ol> <pre><code># OPTIONAL [1]: Add 'set_type' or other columns that shouldn't be passed to the model.\n</code></pre> <p>Remember to remove the pass statement after implementing the first step (Decoding Base64 Message).</p> <p>Deployment:</p> <pre><code>gcloud functions deploy prefix_train_model \\\n--region=europe-west3 \\\n--runtime=python39 \\\n--source=gs://prefix-functions-bucket/train_model.zip \\\n--entry-point=main \\\n--trigger-topic=prefix-ingestion-complete \\\n--memory=1024MB\n</code></pre>"},{"location":"manual_exercises/ex2/#code","title":"Code:","text":"<p>Remember, you can still find it in the correct folder.</p>"},{"location":"manual_exercises/ex2/#manual_exercises.train_model.main","title":"<code>main</code>","text":""},{"location":"manual_exercises/ex3/","title":"3. Create an endpoint to serve the model to the outside world.","text":"<p>In this exercise, you'll be working with the <code>predictions_endpoint</code> Cloud Function. This HTTP-triggered function serves as the prediction endpoint for clients to send new data points. Upon receiving a request containing new data, the function performs the following steps:</p> <ol> <li>It loads the previously trained model from the <code>my-model-storage</code> bucket into memory.</li> <li>Utilizing the loaded model, it generates a prediction based on the new data point received in the request.</li> <li>The function then stores both the prediction and the new data in the <code>predictions_data</code> BigQuery table to maintain a record of all predictions.</li> <li>Finally, it returns the prediction result to the client, completing the request-response cycle.</li> </ol> <p>Your task is to develop the code for the <code>predictions_endpoint</code> Cloud Function and deploy it, ensuring that it can efficiently handle the entire process from receiving new data to returning predictions.</p> <p>The outline of the Cloud Function code is available at <code>./functions/manual_exercises/train_model/</code></p> <ol> <li>Set Bucket Name: Add the GCS bucket name where your model is stored.</li> </ol> <pre><code># IMPLEMENTATION [1]: Add your prefix-bucket-models here\n</code></pre> <ol> <li>Set Model Filename: Provide the name you gave to your model.</li> </ol> <pre><code># IMPLEMENTATION [2]: Put the name you gave your model here\n</code></pre> <ol> <li>Create Storage Client: Use the storage API to make a Client Object.</li> </ol> <pre><code># IMPLEMENTATION [3]: Use the storage API to make a Client Object\n</code></pre> <ol> <li>Connect to Bucket: Connect to the GCS bucket using the correct method for the Storage Client.</li> </ol> <pre><code># IMPLEMENTATION [4]: Connect to the bucket in [4] using the correct method for the storage Client.\n</code></pre> <ol> <li>Connect to Blob: Connect to the blob (file object) inside the bucket, using the bucket object.</li> </ol> <pre><code># IMPLEMENTATION [5]: Connect to the blob(file object) inside the bucket, using the `bucket` object.\n</code></pre> <ol> <li>Make Prediction: Call the predict method of the global pipeline object to make a prediction.</li> </ol> <pre><code># IMPLEMENTATION [6]: You pipeline object is lodaded globally, just call it and use the `predict` method\n</code></pre> <p>Deployment:</p> <pre><code>gcloud functions deploy prefix_predictions_endpoint \\\n--region=europe-west3 \\\n--runtime=python39 \\\n--source=gs://prefix-functions-bucket/predictions_endpoint.zip \\\n--memory=1024MB \\\n--entry-point=predict \\\n--trigger-http \\\n--allow-unauthenticated\n</code></pre> <p>You can make requests with a cURL comamnd like so:</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" -d '{\"Pclass\": 3, \"Name\": \"Some Name\", \"Sex\": \"male\", \"Age\": 22, \"SibSp\": 1, \"Parch\": 0, \"Ticket\": \"A/5 21171\", \"Fare\": 7.25, \"Cabin\": \"\", \"Embarked\": \"S\"}' http://YOUR_FUNCTION_ENDPOINT\n</code></pre> <p>or by going to the app on Stackblitz and change the <code>TitanicEndpoint</code> variable in <code>./src/app/titanic-prediction.service.ts</code>.</p>"},{"location":"manual_exercises/ex3/#code","title":"Code:","text":"<p>Remember, you can still find it in the correct folder.</p>"},{"location":"manual_exercises/ex3/#manual_exercises.predictions_endpoint.main","title":"<code>main</code>","text":""},{"location":"manual_exercises/ex4/","title":"(Extra) Retraining the model.","text":"<p>In this final exercise, you'll focus on extending an existing diagram to include the model retraining cycle. Using a diagramming tool like draw.io or Excalidraw, you will visually represent how the retraining cycle integrates into the current architecture. The diagram should include the following element:</p> <ul> <li>Incorporate the model retraining cycle into the architecture. Show how the system monitors prediction performance, triggers model retraining when necessary, and updates the stored model in the my-model-storage bucket. You may also want to include a mechanism for evaluating the new model's performance and deciding whether to replace the existing model or keep it.</li> </ul> <p>Your task is to design a clear and informative diagram that visually communicates how the model retraining cycle interacts with the existing architecture, helping others better understand the complete system.</p>"},{"location":"manual_exercises/titanic/","title":"Meet the dataset","text":"<p>We will use the Titanic Dataset available pretty much anywhere.</p> <p>The columns and their types are the following</p> Column name Python data type Bigquery data type Description PassengerId int INT64 Unique identifier for each passenger Survived bool BOOLEAN Survival status (False = No, True = Yes) Pclass int INT64 Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd) Name str STRING Full name of the passenger Sex str STRING Gender (male or female) Age float FLOAT64 Age in years SibSp int INT64 Number of siblings/spouses aboard the Titanic Parch int INT64 Number of parents/children aboard the Titanic Ticket str STRING Ticket number Fare float FLOAT64 Passenger fare Cabin str STRING Cabin number Embarked str STRING Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) OPTIONAL: set_type str STRING Set type (Train / Test / Validation) <p>So, when creating the Tables, you have to create the schema accodingly. </p> <p>The dataset is available at <code>./dataset/titanic.csv</code>.</p>"}]}